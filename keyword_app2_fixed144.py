# -*- coding: utf-8 -*-
"""
Keyword Risk Analyzer (v101 â€“ Follow DB column order & values)
- Based on v97
- NEW: ë“œë¡­ë‹¤ìš´ ëª©ë¡ì„ JSON íŒŒì¼ë¡œ ì˜êµ¬ ì €ì¥í•˜ì—¬ ì¬ë¶€íŒ… í›„ì—ë„ ìœ ì§€
- KEEP: ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€ (ì¡°íšŒ ìˆ˜ ì¹´ìš´íŒ…, ë¦¬í¬íŠ¸, ìµœê·¼ 7ì¼ ì´ˆê¸°í™” ë“±)
"""
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Tuple, Set, Optional
import pandas as pd
import streamlit as st

# --- Custom CSS for larger 'í…ìŠ¤íŠ¸ ë¶„ì„í•˜ê¸°' button ---
import streamlit as st
st.markdown(
    """
<style>
div.stButton > button {
    height: 35px !important;
    padding: 12px 20px !important;
    font-size: 1rem !important;
}
</style>
""",
    unsafe_allow_html=True
)
# --- End Custom CSS ---
import streamlit.components.v1 as components
import html, io, re, zipfile, os, json
from xml.etree import ElementTree as ET
from pathlib import Path

st.markdown("""
<style>
div.streamlit-expander {
    margin-bottom: 0rem !important;
}
</style>
""", unsafe_allow_html=True)


from datetime import datetime, timedelta

# Streamlit cache alias (ì§€ì› ë²„ì „ ì°¨ì´ ëŒ€ì‘)
try:
    _cache_data = st.cache_data
except AttributeError:  # Streamlit < 1.18
    _cache_data = st.cache

# Constants
MAX_HIGHLIGHT_HEIGHT = 600  # px
DEFAULT_HEADER_ROW = 1
CONFIG_FILE_NAME = "dropdown_config.json"  # ë“œë¡­ë‹¤ìš´ ì„¤ì • ì €ì¥ íŒŒì¼

st.markdown(
    """
    <style>
    mark {
        background: #ffd43b !important;
        background-color: #ffd43b !important;
        color: #000 !important;
    }
    /* Compact layout ONLY for highlight quick-select area */
    .quick-select-block div[data-testid="stHorizontalBlock"] {
        margin-bottom: 0 !important;
        padding-bottom: 0 !important;
    }
    .quick-select-block div[data-testid="column"] {
        margin-bottom: 0 !important;
        padding-bottom: 0 !important;
    }
    .quick-select-block [data-testid="stVerticalBlock"] {
        gap: 0 !important;
    }
    .quick-select-block div.stButton > button {
        padding-top: 0 !important;
        padding-bottom: 0 !important;
        min-height: 1.1rem !important;
    }
</style>
    """,
    unsafe_allow_html=True
)

# -----------------------------
# XLSX reader without openpyxl (first sheet only, basic types)
# -----------------------------
def _xlsx_list_worksheets(zf: zipfile.ZipFile) -> List[str]:
    paths = [p for p in zf.namelist() if p.startswith("xl/worksheets/") and p.endswith(".xml")]
    return sorted(paths) if paths else []

def _xlsx_load_shared_strings(zf: zipfile.ZipFile) -> List[str]:
    try:
        with zf.open("xl/sharedStrings.xml") as fp:
            tree = ET.parse(fp)
        root = tree.getroot()
        ns = {"a": root.tag.split("}")[0].strip("{")}
        strings = []
        for si in root.findall("a:si", ns):
            parts = [t.text or "" for t in si.findall(".//a:t", ns)]
            strings.append("".join(parts))
        return strings
    except (KeyError, Exception):
        return []

def _xlsx_cell_value(cell, shared_strings: List[str]) -> str:
    t = cell.get("t")
    v = cell.find("./v")
    is_node = cell.find("./is")
    if t == "s":
        if v is not None and v.text is not None:
            try:
                idx = int(v.text)
                return shared_strings[idx] if 0 <= idx < len(shared_strings) else ""
            except Exception:
                return ""
        return ""
    if t == "inlineStr" and is_node is not None:
        parts = [n.text or "" for n in is_node.findall(".//t")]
        return "".join(parts)
    if v is None or v.text is None:
        return ""
    return v.text

def read_xlsx_without_openpyxl(file_bytes: bytes, header_row: Optional[int] = DEFAULT_HEADER_ROW) -> pd.DataFrame:
    """
    Parse first worksheet of an .xlsx file using zipfile + ElementTree.
    Limitations: styles/dates/formulas not evaluated; merged cells not handled.
    """
    with zipfile.ZipFile(io.BytesIO(file_bytes)) as zf:
        sheets = _xlsx_list_worksheets(zf)
        if not sheets:
            raise ValueError("XLSX ë‚´ë¶€ì—ì„œ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        target = sheets[0]
        shared = _xlsx_load_shared_strings(zf)
        with zf.open(target) as fp:
            tree = ET.parse(fp)
        root = tree.getroot()
        ns_uri = root.tag.split("}")[0].strip("{")
        ns = {"a": ns_uri}
        rows = []
        for row in root.findall(".//a:sheetData/a:row", ns):
            values = []
            cells = row.findall("./a:c", ns)
            for c in cells:
                values.append(_xlsx_cell_value(c, shared))
            rows.append(values)

    maxlen = max((len(r) for r in rows), default=0)
    norm = [r + [""] * (maxlen - len(r)) for r in rows]

    if header_row is not None and 1 <= header_row <= len(norm):
        header = [h.strip() for h in norm[header_row - 1]]
        data = norm[header_row:]
        df = pd.DataFrame(data, columns=header)
    else:
        df = pd.DataFrame(norm)
    df = df.fillna("").astype(str)
    return df

# -----------------------------
# Paths
# -----------------------------
def default_storage_path() -> Path:
    try:
        base = Path(__file__).parent
    except NameError:
        base = Path.cwd()
    return base / "keywords_db.csv"

def hits_log_path() -> Path:
    try:
        base = Path(__file__).parent
    except NameError:
        base = Path.cwd()
    return base / "keywords_hits_log.csv"

def get_config_path() -> Path:
    """ë“œë¡­ë‹¤ìš´ ì„¤ì • íŒŒì¼ ê²½ë¡œ"""
    try:
        base = Path(__file__).parent
    except NameError:
        base = Path.cwd()
    return base / CONFIG_FILE_NAME

# -----------------------------
# Constants & defaults
# -----------------------------
DB_COLS = [
    "kwd_no", "í‚¤ì›Œë“œëª…", "ìƒí’ˆì¹´í…Œê³ ë¦¬", "ë¦¬ìŠ¤í¬ ë“±ê¸‰", "ëŒ€ì²´í‚¤ì›Œë“œ",
    "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€", "ì¦ë¹™ìë£Œìœ í˜•",
    "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)", "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì"
]

DISPLAY_F_COL = "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€ (ì…€ì„ ë”ë¸” í´ë¦­í•˜ì—¬ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”)"
RAW_F_COL = "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€"

# ---- Dynamic DB column order helpers ----
def get_db_cols():
    try:
        cols = list(st.session_state.get("db_cols_order") or [])
        if cols:
            return cols
    except Exception:
        pass
    return DB_COLS


def _rename_fcol(df):
    """UI í‘œì‹œìš©ìœ¼ë¡œë§Œ RAW_F_COL â†’ DISPLAY_F_COL í—¤ë”ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""
    try:
        import pandas as _pd
        # Stylerê°€ ë“¤ì–´ì˜¤ë©´ ì›ë³¸ DataFrameì„ ê°€ì ¸ì™€ ì»¬ëŸ¼ë§Œ ë°”ê¾¸ê³  ë‹¤ì‹œ ìŠ¤íƒ€ì¼ ì ìš©
        if hasattr(df, 'to_excel') and hasattr(df, 'style'):
            # DataFrame (ê°€ê¸‰ì  ì´ ë¶„ê¸°ë¡œ)
            return df.rename(columns={RAW_F_COL: DISPLAY_F_COL})
        # pandas Styler ì²˜ë¦¬
        if getattr(df, '__class__', None).__name__ == 'Styler':
            base = getattr(df, 'data', None)
            if base is not None and hasattr(base, 'rename'):
                return base.rename(columns={RAW_F_COL: DISPLAY_F_COL}).style
        # ê·¸ ì™¸ ê°ì²´ë„ columns ì†ì„±ì´ ìˆìœ¼ë©´ ì‹œë„
        if hasattr(df, 'rename') and hasattr(df, 'columns'):
            return df.rename(columns={RAW_F_COL: DISPLAY_F_COL})
        return df
    except Exception:
        return df
DEFAULT_CATEGORIES = ["ê³µí†µ(ì „ì²´)", "ì‹í’ˆ", "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ", "í™”ì¥í’ˆ", "ê³µì‚°í’ˆ"]
CATEGORY_PREFIX = {"ê³µí†µ(ì „ì²´)":"A", "ì‹í’ˆ":"F","ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ":"G","í™”ì¥í’ˆ":"B","ê³µì‚°í’ˆ":"I"}

RISK_OPTIONS = ["1ë“±ê¸‰(ì‚¬ìš©ê¸ˆì§€)","2ë“±ê¸‰(ëŒ€ì²´í‚¤ì›Œë“œì‚¬ìš©)","3ë“±ê¸‰(ì¡°ê±´ë¶€ì‚¬ìš©)","4ë“±ê¸‰(ì‚¬ìš©ê°€ëŠ¥)"]

DEFAULT_DETAIL_CRITERIA = ["-","ì‹¤ì¦ìë£Œì œì¶œ","ì‹œí—˜ì„±ì ì„œì œì¶œ","ê¸°ëŠ¥ì…ì¦ìë£Œì œì¶œ","í‘œì‹œê¸°ì¤€ì¤€ìˆ˜","ì „ë¬¸ì˜ê²¬ì„œ","ë¬¸í—Œìë£Œì œì¶œ"]
DEFAULT_EVIDENCE_TYPES = ["-","ì¸ì²´ì ìš©ì‹œí—˜ê²°ê³¼ì„œ","ê¸°ëŠ¥ì„±í‰ê°€ë³´ê³ ì„œ","ì„ìƒì‹œí—˜ê²°ê³¼ë³´ê³ ì„œ","ì‹¤í—˜ë°ì´í„°ìš”ì•½ì„œ","ì œí’ˆì„±ë¶„ë¶„ì„í‘œ","ë¬¸í—Œìë£Œ","ì‹œí—˜ì„±ì ì„œ"]
DEFAULT_ALT_KEYWORDS = ["-","íƒ„ë ¥","ë³´ìŠµ","ì§„ì •","ë¯¸ë°±","ì£¼ë¦„ê°œì„ ","ìì™¸ì„ ì°¨ë‹¨","ì˜ì–‘ê³µê¸‰"]

RISK_COLORS = {
    "1ë“±ê¸‰(ì‚¬ìš©ê¸ˆì§€)": "#ff6b6b",
    "2ë“±ê¸‰(ëŒ€ì²´í‚¤ì›Œë“œì‚¬ìš©)": "#ffa94d",
    "3ë“±ê¸‰(ì¡°ê±´ë¶€ì‚¬ìš©)": "#ffd43b",
    "4ë“±ê¸‰(ì‚¬ìš©ê°€ëŠ¥)": "#a9e34b",
}

# Regex patterns (compiled once for performance)
_delim_pattern = re.compile(r"[\/,;\|]")
_bracket_pairs = r"\(\)\[\]{}ï¼ˆï¼‰ã€ã€‘"
_kwd_no_pattern = re.compile(r"^([A-Z])(\d{3,})$")
_kwd_split_pattern = re.compile(r"^([A-Za-z]+)(\d+)$")
_custom_kwd_pattern = re.compile(r"^([A-Za-z]+)(\d{1,})$")

# -----------------------------
# Dropdown config persistence (JSON)
# -----------------------------
def load_dropdown_config() -> dict:
    """JSON íŒŒì¼ì—ì„œ ë“œë¡­ë‹¤ìš´ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°"""
    config_path = get_config_path()
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            st.warning(f"ë“œë¡­ë‹¤ìš´ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # íŒŒì¼ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    return {
        "opt_categories": DEFAULT_CATEGORIES.copy(),
        "opt_risks": RISK_OPTIONS.copy(),
        "opt_details": DEFAULT_DETAIL_CRITERIA.copy(),
        "opt_evidences": DEFAULT_EVIDENCE_TYPES.copy(),
        "opt_alt_terms": DEFAULT_ALT_KEYWORDS.copy()
    }

def save_dropdown_config():
    """í˜„ì¬ ë“œë¡­ë‹¤ìš´ ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    normalize_dropdown_lists()
    config_path = get_config_path()
    config = {
        "opt_categories": st.session_state.opt_categories,
        "opt_risks": st.session_state.opt_risks,
        "opt_details": st.session_state.opt_details,
        "opt_evidences": st.session_state.opt_evidences,
        "opt_alt_terms": st.session_state.opt_alt_terms
    }
    try:
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ë“œë¡­ë‹¤ìš´ ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


# -----------------------------
# Dropdown normalize helpers (ko sort + '-' last)
# -----------------------------
def _ko_sorted(seq):
    try:
        import locale
        locale.setlocale(locale.LC_COLLATE, 'ko_KR.UTF-8')
        key = locale.strxfrm
        return sorted(seq, key=key)
    except Exception:
        return sorted(seq)


def unique_values_from_db(col: str) -> list:
    """Return ko-sorted unique, non-empty values for a given column from the current DB."""
    try:
        df = st.session_state.kw_df
        if df is None or df.empty or col not in df.columns:
            return []
        vals = df[col].astype(str).str.strip()
        uniq = [v for v in vals.unique().tolist() if v]
        return _ko_sorted(uniq)
    except Exception:
        return []
def _dedup_keep_order(seq):
    seen = set()
    out = []
    for x in seq:
        x = (x or "").strip()
        if not x:
            continue
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def normalize_dropdown_lists():
    """Ensure details/evidences/alt_terms are ko-sorted and '-' placed last & present."""
    # ëŒ€ìƒ ëª©ë¡: opt_details, opt_evidences, opt_alt_terms
    targets = ["opt_details", "opt_evidences", "opt_alt_terms"]
    for key in targets:
        lst = list(getattr(st.session_state, key, []) or [])
        # clean & dedup
        lst = _dedup_keep_order(lst)
        # remove '-' temporarily
        without_dash = [x for x in lst if x != "-"]
        # ko sort
        without_dash = _ko_sorted(without_dash)
        # append '-' at the end (guarantee presence)
        without_dash.append("-")
        setattr(st.session_state, key, without_dash)


def _dropdown_with_input_option(lst):
    """Return normalized list for selectbox: ko-sorted with '-' last, plus '(ì§ì ‘ ì…ë ¥)' at the end."""
    tmp = _dedup_keep_order(lst)
    tmp = [x for x in tmp if x != "-"]
    tmp = _ko_sorted(tmp)
    tmp.append("-")
    return tmp + ["(ì§ì ‘ ì…ë ¥)"]

# -----------------------------
# Persistence helpers (with encoding fallbacks)
# -----------------------------
def read_csv_with_fallback_bytes(raw: bytes) -> pd.DataFrame:
    last_err = None
    for enc in ["utf-8-sig", "utf-8", "cp949", "euc-kr", "latin1"]:
        try:
            return pd.read_csv(io.BytesIO(raw), dtype=str, encoding=enc).fillna("")
        except Exception as e:
            last_err = e
            continue
    raise last_err

def _ensure_counter_columns(df: pd.DataFrame) -> pd.DataFrame:
    if "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)" not in df.columns:
        df["í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"] = "0"
    if "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì" not in df.columns:
        df["ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì"] = ""
    if "í‚¤ì›Œë“œ ë“±ë¡ì¼ì" not in df.columns:
        df["í‚¤ì›Œë“œ ë“±ë¡ì¼ì"] = ""
    return df

def load_db(path: Path) -> pd.DataFrame:
    if path.exists():
        try:
            raw = path.read_bytes()
            df = read_csv_with_fallback_bytes(raw)
            for c in DB_COLS:
                if c not in df.columns:
                    df[c] = "0" if c == "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)" else ""
            df = _ensure_counter_columns(df)
            return df
        except Exception as e:
            st.warning(f"ì €ì¥ëœ DBë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    df = pd.DataFrame(columns=DB_COLS)
    df = _ensure_counter_columns(df)
    return df

def save_db(df: pd.DataFrame, path: Path) -> None:
    from pathlib import Path as _P


    try:
        _P(path).parent.mkdir(parents=True, exist_ok=True)
        sorted_df = sort_db_internal(df)
        sorted_df[get_db_cols()].to_csv(path, index=False, encoding="utf-8-sig")
    except Exception as e:
        st.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def append_hits_log(kwd_list: List[str]) -> None:
    if not kwd_list:
        return
    log_p = hits_log_path()
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    rows = [{"ts": ts, "kwd_no": k} for k in kwd_list if k]
    df = pd.DataFrame(rows, columns=["ts","kwd_no"])
    try:
        if log_p.exists():
            df.to_csv(log_p, mode="a", header=False, index=False, encoding="utf-8-sig")
        else:
            df.to_csv(log_p, mode="w", header=True, index=False, encoding="utf-8-sig")
    except Exception as e:
        st.warning(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    # ì¡°íšŒ ë¡œê·¸ê°€ ë³€ê²½ë˜ë©´ ìºì‹œ ë¬´íš¨í™”
    try:
        _cache_data.clear()
    except Exception:
        pass

@_cache_data(show_spinner=False)
def load_hits_log() -> pd.DataFrame:
    log_p = hits_log_path()
    if not log_p.exists():
        return pd.DataFrame(columns=["ts","kwd_no"])
    try:
        return pd.read_csv(log_p, dtype=str, encoding="utf-8-sig").fillna("")
    except Exception:
        return pd.DataFrame(columns=["ts","kwd_no"])

def overwrite_hits_log(df: pd.DataFrame) -> None:
    """Overwrite the hits log CSV safely with utf-8-sig encoding."""
    log_p = hits_log_path()
    try:
        df.to_csv(log_p, mode="w", header=True, index=False, encoding="utf-8-sig")
    except Exception as e:
        st.error(f"ì¡°íšŒ ë¡œê·¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    # ì „ì²´ ë¡œê·¸ë¥¼ ë®ì–´ì¼ìœ¼ë¯€ë¡œ ìºì‹œ ë¬´íš¨í™”
    try:
        _cache_data.clear()
    except Exception:
        pass

# -----------------------------
# Sorting helpers
# -----------------------------

def _split_kwd_series(s):
    s = s.astype(str)
    m = s.str.extract(_kwd_split_pattern)
    pref = m[0].fillna("")
    num = m[1].fillna("0").astype(int)
    return pref, num

def sort_db_internal(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "kwd_no" not in df.columns:
        return df
    pref, num = _split_kwd_series(df["kwd_no"])
    sorted_df = (
        df.assign(_pref=pref, _num=num)
        .sort_values(by=["ìƒí’ˆì¹´í…Œê³ ë¦¬", "_pref", "_num", "í‚¤ì›Œë“œëª…"], kind="mergesort")
        .drop(columns=["_pref", "_num"])
        .reset_index(drop=True)
    )
    return sorted_df

def sort_for_display(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "kwd_no" not in df.columns:
        return df
    pref, num = _split_kwd_series(df["kwd_no"])
    sorted_df = (
        df.assign(_pref=pref, _num=num)
        .sort_values(by=["_pref", "_num", "í‚¤ì›Œë“œëª…"], kind="mergesort")
        .drop(columns=["_pref", "_num"])
        .reset_index(drop=True)
    )
    return sorted_df

# -----------------------------
# Data model for matches
# -----------------------------
@dataclass
class Match2:
    term: str
    start: int
    end: int
    category: str = ""
    risk: str = ""
    detail: str = ""
    kwd_no: str | None = None

# -----------------------------
# Session init & numbering helpers
# -----------------------------
def init_state():
    if "storage_path" not in st.session_state:
        st.session_state.storage_path = str(default_storage_path())
    if "kw_df" not in st.session_state:
        st.session_state.kw_df = load_db(Path(st.session_state.storage_path))
        # Record DB column order as loaded
        try:
            st.session_state.db_cols_order = list(st.session_state.kw_df.columns)
        except Exception:
            st.session_state.db_cols_order = list(DB_COLS)
    
    # ë“œë¡­ë‹¤ìš´ ì„¤ì •ì„ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    if "opt_categories" not in st.session_state:
        config = load_dropdown_config()
        st.session_state.opt_categories = config["opt_categories"]
        st.session_state.opt_risks = config["opt_risks"]
        st.session_state.opt_details = config["opt_details"]
        st.session_state.opt_evidences = config["opt_evidences"]
        st.session_state.opt_alt_terms = config["opt_alt_terms"]
        normalize_dropdown_lists()
    
    if "counters" not in st.session_state:
        st.session_state.counters = {}

def scan_existing_counters():
    for v in st.session_state.kw_df["kwd_no"].dropna().astype(str):
        m = _kwd_no_pattern.match(v.strip())
        if not m:
            continue
        pfx, num = m.group(1), int(m.group(2))
        st.session_state.counters[pfx] = max(st.session_state.counters.get(pfx, 0), num)

def get_prefix(category: str) -> str:
    if category in CATEGORY_PREFIX:
        return CATEGORY_PREFIX[category]
    first = (category[:1] or "U").upper()
    return first if re.match(r"[A-Z]", first) else "U"

def infer_prefix_from_existing(category: str) -> str:
    df = st.session_state.kw_df
    if df is not None and not df.empty:
        sub = df[df["ìƒí’ˆì¹´í…Œê³ ë¦¬"].astype(str).str.strip() == str(category).strip()]["kwd_no"].dropna().astype(str)
        if not sub.empty:
            pref = sub.str.extract(r"^([A-Z])(\d{3,})$")[0].dropna()
            if not pref.empty:
                return pref.value_counts().idxmax()
    return get_prefix(category)

def next_kwd_no(category: str) -> str:
    pfx = infer_prefix_from_existing(category)
    df = st.session_state.kw_df
    next_num = None
    if df is not None and not df.empty:
        sub = df[df["ìƒí’ˆì¹´í…Œê³ ë¦¬"].astype(str).str.strip() == str(category).strip()]["kwd_no"].astype(str)
        nums = sub.str.extract(r"^%s(\d{3,})$" % pfx)[0].dropna()
        if not nums.empty:
            next_num = int(nums.astype(int).max()) + 1
    if next_num is None:
        cur = st.session_state.counters.get(pfx, 0) + 1
        st.session_state.counters[pfx] = cur
        next_num = cur
    else:
        st.session_state.counters[pfx] = max(st.session_state.counters.get(pfx, 0), next_num)
    return f"{pfx}{next_num:03d}"

def normalize_upload(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d.columns = [str(c).strip().lower() for c in d.columns]
    mapping = {
        "kwd_no": "kwd_no",
        "keyword_no": "kwd_no",
        "í‚¤ì›Œë“œno": "kwd_no",
        "í‚¤ì›Œë“œ no": "kwd_no",
        "í‚¤ì›Œë“œëª…": "í‚¤ì›Œë“œëª…",
        "term": "í‚¤ì›Œë“œëª…",
        "ìƒí’ˆì¹´í…Œê³ ë¦¬": "ìƒí’ˆì¹´í…Œê³ ë¦¬",
        "category": "ìƒí’ˆì¹´í…Œê³ ë¦¬",
        "ë¦¬ìŠ¤í¬ ë“±ê¸‰": "ë¦¬ìŠ¤í¬ ë“±ê¸‰",
        "risk": "ë¦¬ìŠ¤í¬ ë“±ê¸‰",
        "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€": "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€",
        "ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€": "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€",
        "ì¦ë¹™ìë£Œìœ í˜•": "ì¦ë¹™ìë£Œìœ í˜•",
        "ì¦ë¹™ìë£Œ": "ì¦ë¹™ìë£Œìœ í˜•",
        "ëŒ€ì²´í‚¤ì›Œë“œ": "ëŒ€ì²´í‚¤ì›Œë“œ",
        "ëŒ€ì²´í‚¤ì›Œë“œëª…": "ëŒ€ì²´í‚¤ì›Œë“œ"
    }
    rename_dict = {col: mapping[col] for col in d.columns if col in mapping}
    if rename_dict:
        d = d.rename(columns=rename_dict)
    for col in DB_COLS:
        if col not in d.columns:
            if col == "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)":
                d[col] = "0"
            elif col == "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì":
                d[col] = ""
            else:
                d[col] = "" if col != "kwd_no" else None
    d = d[get_db_cols()]
    for c in d.columns:
        d[c] = d[c].astype(str).str.strip()
    return d

# -----------------------------
# Variant generation for partial matching
# -----------------------------
def generate_keyword_variants(term: str) -> list:
    if term is None:
        return []
    t = str(term).strip()
    if not t:
        return []
    variants = set()

    def add(v: str):
        v = (v or "").strip()
        if not v:
            return
        if re.search(r"[A-Za-z]", v):
            if len(v) < 3:
                return
        else:
            if len(v) < 2:
                return
        variants.add(v)

    add(t)
    for piece in _delim_pattern.split(t):
        add(piece)
    outside = re.sub(r"\s*[\(\[ï¼ˆã€].*?[\)\]ï¼‰ã€‘]\s*", " ", t).strip()
    add(outside)
    for inner in re.findall(r"[\(\[ï¼ˆã€](.*?)[\)\]ï¼‰ã€‘]", t):
        add(inner)
        for piece in _delim_pattern.split(inner):
            add(piece)

    def _leading_core(s: str):
        s = (s or "").strip()
        if not s:
            return None
        m_ko = re.match(r'^[ê°€-í£]{3,}', s)
        if m_ko:
            return m_ko.group(0)[:3]
        m_en = re.match(r'^[A-Za-z]{4,}', s)
        if m_en:
            return m_en.group(0)[:4]
        return None
    for _v in list(variants):
        _lc = _leading_core(_v)
        if _lc:
            add(_lc)

    def _add_prefixes(s: str):
        s = (s or "").strip()
        if not s:
            return
        m_ko = re.match(r'^[ê°€-í£]+', s)
        if m_ko:
            ko = m_ko.group(0)
            if len(ko) >= 2:
                add(ko[:2])
            if len(ko) >= 3:
                add(ko[:3])
        m_en = re.match(r'^[A-Za-z]+', s)
        if m_en:
            en = m_en.group(0)
            if len(en) >= 3:
                add(en[:3].lower())
            if len(en) >= 4:
                add(en[:4].lower())
    for _v in list(variants):
        _add_prefixes(_v)
    variants = {re.sub(r"\s+", " ", v) for v in variants}
    return list(variants)

# -----------------------------
# Matching & highlight
# -----------------------------
def find_matches(text: str, kw_df: pd.DataFrame, match_mode: str = "ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨", exact_relaxed: bool = False, **kwargs) -> List[Match2]:
    matches: List[Match2] = []
    base = text or ""
    lower = base.lower()

    df = kw_df.copy()
    for col in DB_COLS:
        if col not in df.columns:
            df[col] = ""
    df = df[df["í‚¤ì›Œë“œëª…"].astype(str).str.strip() != ""]

    occupied_set = set()
    records = df.to_dict("records")

    for r in records:
        term = str(r["í‚¤ì›Œë“œëª…"]).strip()
        category = str(r.get("ìƒí’ˆì¹´í…Œê³ ë¦¬",""))
        risk = str(r.get("ë¦¬ìŠ¤í¬ ë“±ê¸‰",""))
        detail = str(r.get("ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€",""))
        kwd_no = r.get("kwd_no") or None

        variants = generate_keyword_variants(term)
        if match_mode == "ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨" and exact_relaxed:
            term_no_br = re.sub(r"\s*[\(\[ï¼ˆã€].*?[\)\]ï¼‰ã€‘]\s*", " ", str(term)).strip()
            term_no_br = re.sub(r"\s+", " ", term_no_br)
            term_norm = re.sub(r"\s+", " ", str(term)).strip()
            for _cand in [term_no_br, term_norm]:
                if _cand and _cand not in variants:
                    variants.insert(0, _cand)

        if match_mode == "ì •í™• ì¼ì¹˜":
            variants = [str(term)]
            if exact_relaxed:
                term_no_br = re.sub(r"\s*[\(\[ï¼ˆã€].*?[\)\]ï¼‰ã€‘]\s*", " ", str(term)).strip()
                term_no_br = re.sub(r"\s+", " ", term_no_br)
                term_norm = re.sub(r"\s+", " ", str(term)).strip()
                for _cand in [term_no_br, term_norm]:
                    if _cand and _cand not in variants:
                        variants.append(_cand)

        variants.sort(key=lambda s: len(s), reverse=True)

        for v in variants:
            v_lower = v.lower()
            start = 0
            while True:
                idx = lower.find(v_lower, start)
                if idx == -1:
                    break
                end = idx + len(v)

                for _i_m, _m in enumerate(list(matches)):
                    if getattr(_m, 'start', None) == idx and getattr(_m, 'end', 0) < end:
                        for _j in range(_m.start, _m.end):
                            occupied_set.discard(_j)
                        try:
                            matches.pop(_i_m)
                        except Exception:
                            pass

                range_occupied = any(i in occupied_set for i in range(idx, end))

                if not range_occupied:
                    matches.append(Match2(term=term, start=idx, end=end, category=category, risk=risk, detail=detail, kwd_no=kwd_no))
                    for i in range(idx, end):
                        occupied_set.add(i)
                else:
                    same_span_exists = any((m.start == idx and m.end == end) for m in matches)
                    already_same = any((m.start == idx and m.end == end and (m.kwd_no == kwd_no and m.category == category and m.risk == risk)) for m in matches)
                    if same_span_exists and not already_same:
                        matches.append(Match2(term=term, start=idx, end=end, category=category, risk=risk, detail=detail, kwd_no=kwd_no))

                start = end

    matches.sort(key=lambda m: (m.start, m.end))
    return matches



# -----------------------------
# NLP-enhanced matching wrapper (spacing-insensitive)
# -----------------------------
def _build_compact_index(src_text: str):
    """
    Build a whitespace-removed version of the text and an index map
    from compact index -> original index.
    """
    if not src_text:
        return "", []
    compact_chars = []
    index_map = []
    for i, ch in enumerate(src_text):
        if ch.isspace():
            continue
        compact_chars.append(ch)
        index_map.append(i)
    return "".join(compact_chars), index_map


def _spacing_insensitive_matches(
    text: str,
    kw_df: pd.DataFrame,
    match_mode: str = "ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨",
    exact_relaxed: bool = False,
) -> List[Match2]:
    """
    ì¶”ê°€ NLP ë¡œì§ (1ë‹¨ê³„):
    - í‚¤ì›Œë“œì™€ ë³¸ë¬¸ì—ì„œ **ëª¨ë“  ê³µë°±ì„ ì œê±°í•œ ìƒíƒœ**ë¡œë„ í•œ ë²ˆ ë” ë§¤ì¹­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - ë„ì–´ì“°ê¸° ì˜¤ë¥˜/ë³€í˜•ìœ¼ë¡œ ì¸í•´ ë†“ì¹˜ëŠ” ì¼€ì´ìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.
    - ê¸°ë³¸ find_matches ê²°ê³¼ì— **ì¶”ê°€**ë¡œë§Œ ì‚¬ìš©ë˜ë©°, ê¸°ì¡´ ë¡œì§ì„ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    if not text or kw_df is None or kw_df.empty:
        return []

    # í˜„ì¬ëŠ” "ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨" ëª¨ë“œì—ì„œë§Œ ë™ì‘í•˜ë„ë¡ ì œí•œ (ê¸°ì¡´ ë™ì‘ ì˜í–¥ ìµœì†Œí™”)
    if match_mode != "ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨":
        return []

    base = text or ""
    compact_text, index_map = _build_compact_index(base)
    if not compact_text:
        return []

    # DB ì»¬ëŸ¼ ì •í•©ì„± ë§ì¶”ê¸°
    df = kw_df.copy()
    for col in DB_COLS:
        if col not in df.columns:
            df[col] = ""
    df = df[df["í‚¤ì›Œë“œëª…"].astype(str).str.strip() != ""]
    records = df.to_dict("records")

    extra_matches: List[Match2] = []

    for r in records:
        term = str(r["í‚¤ì›Œë“œëª…"]).strip()
        if not term:
            continue
        category = str(r.get("ìƒí’ˆì¹´í…Œê³ ë¦¬", ""))
        risk = str(r.get("ë¦¬ìŠ¤í¬ ë“±ê¸‰", ""))
        detail = str(r.get("ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€", ""))
        kwd_no = r.get("kwd_no") or None

        # ê¸°ì¡´ ë³€í˜• ìƒì„± ë¡œì§ ì¬ì‚¬ìš©
        variants = generate_keyword_variants(term)
        if not variants:
            variants = [term]

        for v in variants:
            v = str(v or "").strip()
            if not v:
                continue
            # ê³µë°± ì œê±° ë²„ì „ìœ¼ë¡œ ë¹„êµ
            v_compact = re.sub(r"\s+", "", v)
            if not v_compact:
                continue

            start_pos = 0
            while True:
                idx = compact_text.find(v_compact, start_pos)
                if idx < 0:
                    break
                end_idx_compact = idx + len(v_compact) - 1
                if end_idx_compact >= len(index_map):
                    break

                start_orig = index_map[idx]
                end_orig = index_map[end_idx_compact] + 1  # exclusive

                extra_matches.append(
                    Match2(
                        term=term,
                        start=start_orig,
                        end=end_orig,
                        category=category,
                        risk=risk,
                        detail=detail,
                        kwd_no=kwd_no,
                    )
                )
                start_pos = idx + 1

    # ì¤‘ë³µ/ê²¹ì¹¨ì€ ìƒìœ„ ë˜í¼ì—ì„œ ì •ë¦¬
    extra_matches.sort(key=lambda m: (m.start, m.end))
    return extra_matches


def find_matches_nlp(
    text: str,
    kw_df: pd.DataFrame,
    match_mode: str = "ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨",
    exact_relaxed: bool = False,
    **kwargs,
) -> List[Match2]:
    """
    ê¸°ì¡´ find_matchesì— **ê°„ë‹¨í•œ NLP ë³´ì •(ë„ì–´ì“°ê¸° ë¬´ì‹œ ë§¤ì¹­)**ì„ ì–¹ì€ ë˜í¼ì…ë‹ˆë‹¤.

    - 1ì°¨: ê¸°ì¡´ find_matches ë¡œì§ ê·¸ëŒ€ë¡œ ìˆ˜í–‰ (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    - 2ì°¨: _spacing_insensitive_matches ë¡œ ì¶”ê°€ í›„ë³´ë¥¼ ì°¾ìŒ
    - 3ì°¨: (term, start, end, kwd_no) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° í›„ ë³‘í•©

    í–¥í›„ í˜•íƒœì†Œ ë¶„ì„ / ì˜¤íƒ€ ë³´ì • ë¡œì§ì„ ì´ í•¨ìˆ˜ ì•ˆì— ë‹¨ê³„ì ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    base_matches = find_matches(
        text,
        kw_df,
        match_mode=match_mode,
        exact_relaxed=exact_relaxed,
        **kwargs,
    )

    try:
        extra_matches = _spacing_insensitive_matches(
            text,
            kw_df,
            match_mode=match_mode,
            exact_relaxed=exact_relaxed,
        )
    except Exception:
        extra_matches = []

    merged: List[Match2] = []
    # ë¨¼ì € ê¸°ì¡´ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë„£ê³ 
    for m in base_matches or []:
        merged.append(m)

    # ì¶”ê°€ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
    def _same(a: Match2, b: Match2) -> bool:
        return (
            a.start == b.start
            and a.end == b.end
            and str(a.term) == str(b.term)
            and (a.kwd_no or "") == (b.kwd_no or "")
        )

    for m in extra_matches:
        if any(_same(m, ex) for ex in merged):
            continue
        merged.append(m)

    merged.sort(key=lambda m: (m.start, m.end))
    return merged

def highlight_text(text: str, matches: List[Match2]) -> str:
    if not matches:
        return html.escape(text)

    def _risk_grade(r: str) -> int:
        r = (r or '').strip()
        if r.startswith('1'): return 1
        if r.startswith('2'): return 2
        if r.startswith('3'): return 3
        if r.startswith('4'): return 4
        return 0

    text_colors = {1:'#e03131', 2:'#1c7ed6', 3:'#1c7ed6', 4:'#2f9e44'}
    seg_info = {}
    for m in matches:
        seg = text[m.start:m.end]
        if not seg:
            continue
        info = seg_info.get(seg)
        if info is None:
            info = {'ids': [], 'first': m.start, 'risk_map': {}}
            seg_info[seg] = info
        if m.start < info['first']:
            info['first'] = m.start
        if m.kwd_no and m.kwd_no not in info['ids']:
            info['ids'].append(m.kwd_no)
        g = _risk_grade(m.risk)
        if m.kwd_no:
            info['risk_map'][m.kwd_no] = g

    if not seg_info:
        return html.escape(text)

    ordered = sorted(seg_info.items(), key=lambda kv: (kv[1]['first'], -len(kv[0])))
    working = text
    tokens = []
    for i, (seg, info) in enumerate(ordered):
        token = f"__HL_TOKEN_{i}__"
        tokens.append((token, seg, info['ids'], info['risk_map']))
        working = working.replace(seg, token)

    escaped = html.escape(working)

    for token, seg, ids, risk_map in tokens:
        frag = []
        for kid in ids:
            g = risk_map.get(kid, 0)
            color = text_colors.get(g, 'inherit')
            frag.append(f"<span class='kwdno-link' data-kwd='{html.escape(kid)}' style='color:{color}; text-decoration: underline; cursor:pointer'><b>{html.escape(kid)}</b></span>")
        ids_html = ", ".join(frag)

        base_style = "background:#ffd43b !important; background-color:#ffd43b !important; color:#000;"
        risk_set = {g for g in risk_map.values() if g}
        if len(risk_set) == 1:
            if risk_set == {1}:
                mark_style = f" style='{base_style} background:#ffd6d6 !important; background-color:#ffd6d6 !important;'"
            elif risk_set == {2}:
                mark_style = f" style='{base_style} background:#d0ebff !important; background-color:#d0ebff !important;'"
            elif risk_set == {3}:
                mark_style = f" style='{base_style} background:#d0ebff !important; background-color:#d0ebff !important;'"
            elif risk_set == {4}:
                mark_style = f" style='{base_style} background:#d3f9d8 !important; background-color:#d3f9d8 !important;'"
            else:
                mark_style = f" style='{base_style}'"
        else:
            mark_style = f" style='{base_style}'"

        leading_ws = seg[:len(seg) - len(seg.lstrip())]
        trailing_ws = seg[len(seg.rstrip()):]
        core = seg.strip()
        decorated = f"{leading_ws}<mark{mark_style}>ã€<b>{html.escape(core)}</b>ã€‘({ids_html})</mark>{trailing_ws}"
        escaped = escaped.replace(html.escape(token), decorated)

    return escaped

# -----------------------------
# Upload merge helpers (needed for v97)
# -----------------------------
def assign_missing_ids(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for i, row in out.iterrows():
        if not str(row.get("kwd_no", "")).strip():
            cat = str(row.get("ìƒí’ˆì¹´í…Œê³ ë¦¬", "")).strip() or "ê³µí†µ(ì „ì²´)"
            out.at[i, "kwd_no"] = next_kwd_no(cat)
    return out

def merge_or_overwrite(base: pd.DataFrame, incoming: pd.DataFrame, mode: str) -> Tuple[pd.DataFrame, int, int]:
    inc = incoming.copy()
    before = len(inc)
    if "kwd_no" in inc.columns and inc["kwd_no"].astype(str).str.strip().any():
        inc = inc.sort_values(by=["kwd_no"]).drop_duplicates(subset=["kwd_no"], keep="last")
    else:
        inc = inc.sort_values(by=["í‚¤ì›Œë“œëª…", "ìƒí’ˆì¹´í…Œê³ ë¦¬"]).drop_duplicates(subset=["í‚¤ì›Œë“œëª…", "ìƒí’ˆì¹´í…Œê³ ë¦¬"], keep="last")
    dedup_removed = before - len(inc)

    if mode == "overwrite":
        return inc.reset_index(drop=True), len(inc), dedup_removed

    if base.empty:
        return inc.reset_index(drop=True), len(inc), dedup_removed

    if inc["kwd_no"].astype(str).str.strip().any():
        new_ids: Set[str] = set(inc["kwd_no"].astype(str))
        base = base[~base["kwd_no"].astype(str).isin(new_ids)]

    merged = pd.concat([base, inc], ignore_index=True)
    return merged, len(inc), dedup_removed

# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title="Keyword Risk Analyzer", layout="wide")

# --- Compact button & row spacing ---
st.markdown("""
<style>
/* Reduce vertical gaps between rows of columns (affects button grids) */
div[data-testid="stHorizontalBlock"] {
    margin-bottom: 0rem;
}
/* Make buttons more compact (less vertical padding) */
div.stButton > button {
    padding-top: 0rem;
    padding-bottom: 0rem;
    min-height: 1.2rem;
}
</style>
""", unsafe_allow_html=True)
st.title("ğŸ” ê´‘ê³ ì‹¬ì˜ ìë™í™” ì†”ë£¨ì…˜(ê°€ì¹­)")

# --- Scroll margin for keyword anchor links ---
st.markdown("""
<style>
[id^="G"], [id^="F"], [id^="P"], [id^="K"] {
  scroll-margin-top: 100px;
}
</style>
""", unsafe_allow_html=True)

# --- Minimal wrap CSS (safe) ---
st.markdown(
    """
    <style>
    [data-testid="stDataFrame"] [role="gridcell"] {
        white-space: normal !important;
        word-break: break-word !important;
    }
    </style>
    """, unsafe_allow_html=True
)

st.markdown(
    f"""
<style>
.hl-box{{
  border:1px solid rgba(49,51,63,0.2);
  border-radius:8px;
  padding:12px;
  max-height:{MAX_HIGHLIGHT_HEIGHT}px;
  overflow-y:auto;
  overflow-x:auto;
  background:#fff;
  white-space:pre-wrap;
  line-height:1.5;
  box-sizing:border-box;
}}
.hl-box mark{{padding:0;border-radius:2px}}
[data-testid="stDataFrame"] div{{white-space:normal !important;}}
</style>
""",
    unsafe_allow_html=True,
)

init_state()
scan_existing_counters()

# Sidebar: ì €ì¥ ê²½ë¡œ ë° ìœ í‹¸ (ì´ˆê¸°í™” UIëŠ” ì œê±°ë¨)
st.sidebar.header("ğŸ—‚ ë°ì´í„° ì €ì¥")
st.sidebar.caption("CSVë¡œ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ ë¬¸ì œ ì‹œ OneDrive ì™¸ë¶€ ê²½ë¡œ ê¶Œì¥)")
st.sidebar.write("í˜„ì¬ ê²½ë¡œ:")
st.sidebar.code(st.session_state.storage_path, language="text")
new_path = st.sidebar.text_input("ì €ì¥ ê²½ë¡œ ë³€ê²½", value=st.session_state.storage_path)
c_sb1, c_sb2 = st.sidebar.columns(2)
if c_sb1.button("ê²½ë¡œ ì ìš©", key="apply_path"):
    st.session_state.storage_path = new_path
    st.session_state.kw_df = load_db(Path(st.session_state.storage_path))
    scan_existing_counters()
    st.sidebar.success("ê²½ë¡œ ì ìš© ë° DB ë¡œë“œ ì™„ë£Œ")
if c_sb2.button("ê°•ì œ ì €ì¥", key="force_save"):
    save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
    st.sidebar.success("ì €ì¥ ì™„ë£Œ")



# --- ê´€ë¦¬ì ë¡œê·¸ì¸ ì˜ì—­ (í‚¤ì›Œë“œ ê´€ë¦¬ íƒ­ ë¹„ê³µê°œìš©) ---
def render_admin_login():
    """ì‚¬ì´ë“œë°”ì—ì„œ ê´€ë¦¬ì ê³„ì • ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒì„ ì²˜ë¦¬í•˜ê³ , st.session_state.is_admin í”Œë˜ê·¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤."""
    if "is_admin" not in st.session_state:
        st.session_state.is_admin = False

    admin_user = None
    admin_password = None

    # 1) Streamlit secretsì—ì„œ ì‹œë„
    try:
        admin_user = st.secrets.get("ADMIN_USER", None)
        admin_password = st.secrets.get("ADMIN_PASSWORD", None)
    except Exception:
        # secrets ë¯¸ì„¤ì • í™˜ê²½ ë“± ì˜ˆì™¸ëŠ” ë¬´ì‹œ
        pass

    # 2) í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë³´ì¡°ë¡œ ì‹œë„ (ê¹ƒí—ˆë¸Œ ê³µê°œ ì €ì¥ì†Œì— ë¹„ë°€ë²ˆí˜¸ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê¸° ìœ„í•¨)
    if admin_user is None:
        admin_user = os.environ.get("ADMIN_USER")
    if admin_password is None:
        admin_password = os.environ.get("ADMIN_PASSWORD")

    with st.sidebar.expander("ğŸ” ê´€ë¦¬ì ë¡œê·¸ì¸", expanded=False):
        st.caption("í‚¤ì›Œë“œ ê´€ë¦¬ íƒ­ì€ ê´€ë¦¬ì ì „ìš©ì…ë‹ˆë‹¤. ê´€ë¦¬ìë§Œ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì•Œê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
        input_id = st.text_input("ê´€ë¦¬ì ID", key="admin_id")
        input_pw = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password", key="admin_pw")

        c_login, c_logout = st.columns(2)
        with c_login:
            if st.button("ë¡œê·¸ì¸", key="admin_login"):
                ok = False
                if admin_password:
                    # ADMIN_USERê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ID+PW ëª¨ë‘ ê²€ì‚¬, ì•„ë‹ˆë©´ PWë§Œ ê²€ì‚¬
                    if admin_user:
                        ok = (input_id == admin_user and input_pw == admin_password)
                    else:
                        ok = (input_pw == admin_password)
                if ok:
                    st.session_state.is_admin = True
                    st.success("ê´€ë¦¬ì ëª¨ë“œë¡œ ì ‘ì†ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.session_state.is_admin = False
                    st.error("ê´€ë¦¬ì ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ID/ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        with c_logout:
            if st.button("ë¡œê·¸ì•„ì›ƒ", key="admin_logout"):
                st.session_state.is_admin = False
                st.info("ë¡œê·¸ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

def is_admin() -> bool:
    """í˜„ì¬ ì„¸ì…˜ì´ ê´€ë¦¬ì ëª¨ë“œì¸ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return bool(st.session_state.get("is_admin", False))


render_admin_login()

tab1, tab3, tab2 = st.tabs(["ë¶„ì„í•˜ê¸°", "ë¦¬í¬íŠ¸(ì£¼ê°„ Top N)", "í‚¤ì›Œë“œ ê´€ë¦¬"])

with tab1:
    # â‘  í…ìŠ¤íŠ¸ ì…ë ¥
    st.subheader("â‘  í…ìŠ¤íŠ¸ ì…ë ¥")
    sample = "ì´ ë¬¸ì¥ì—ëŠ” í•„ëŸ¬ì™€ ì•ˆí‹°ì—ì´ì§•ì´ë¼ëŠ” í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    text = st.text_area("ë¶„ì„í•  í…ìŠ¤íŠ¸", value=sample, height=180)

    st.markdown("---")
    # â‘¡) ë¶„ì„ ì‹¤í–‰
    st.subheader("â‘¡ ë¶„ì„ ì‹¤í–‰")
    with st.expander("ì‚¬ì „ í•„í„° (ì„ íƒ)", expanded=True):
        cf1, cf2, cf3 = st.columns([1,1,1])
        with cf1:
            db_cats = unique_values_from_db("ìƒí’ˆì¹´í…Œê³ ë¦¬")
            pre_cats = st.multiselect("ìƒí’ˆì¹´í…Œê³ ë¦¬", options=db_cats, key="pre_cats")
        with cf2:
            db_risks = unique_values_from_db("ë¦¬ìŠ¤í¬ ë“±ê¸‰")
            pre_risks = st.multiselect("ë¦¬ìŠ¤í¬ ë“±ê¸‰", options=db_risks, key="pre_risks")
        with cf3:
            match_mode = st.radio("ë§¤ì¹­ ì¡°ê±´", ["ìœ ì‚¬ í‚¤ì›Œë“œ í¬í•¨", "ì •í™• ì¼ì¹˜"], index=1, key="match_mode")
            exact_relaxed = st.checkbox("(ê´„í˜¸ ì œê±°Â·ê³µë°± ì •ê·œí™”Â·ì˜ë¬¸ ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)", value=True, key="exact_relaxed")
    
    if st.button("í…ìŠ¤íŠ¸ ë¶„ì„í•˜ê¸°", type="primary", key="btn_analyze"):
        try:
            if st.session_state.kw_df.empty:
                st.info("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¶”ê°€í•˜ì„¸ìš”.")
            else:
                matches = find_matches_nlp(
                    text,
                    st.session_state.kw_df,
                    match_mode=st.session_state.match_mode,
                    exact_relaxed=st.session_state.exact_relaxed
                )
                if st.session_state.get("pre_cats"):
                    matches = [m for m in matches if str(m.category).strip() in set(st.session_state.pre_cats)]
                if st.session_state.get("pre_risks"):
                    matches = [m for m in matches if str(m.risk).strip() in set(st.session_state.pre_risks)]

                if matches:
                    out = pd.DataFrame(
                        [
                            {
                                "kwd_no": m.kwd_no,
                                "í‚¤ì›Œë“œëª…": m.term,
                                "ìƒí’ˆì¹´í…Œê³ ë¦¬": m.category,
                                "ë¦¬ìŠ¤í¬ ë“±ê¸‰": m.risk,
                                "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€": m.detail,
                                "start": m.start,
                                "end": m.end,
                            }
                            for m in matches
                        ]
                    )
                    base_cols = ["kwd_no", "í‚¤ì›Œë“œëª…", "ìƒí’ˆì¹´í…Œê³ ë¦¬", "ë¦¬ìŠ¤í¬ ë“±ê¸‰", "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€"]
                    display_unique = out[base_cols].drop_duplicates(subset=["kwd_no"]).reset_index(drop=True)
                    display_unique = display_unique.sort_values(by="kwd_no", ascending=True).reset_index(drop=True)

                    # ëˆ„ì  ì¹´ìš´íŒ… + ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì + ë¡œê·¸ ê¸°ë¡
                    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    unique_kwds = display_unique["kwd_no"].dropna().astype(str).str.strip().tolist()
                    if unique_kwds:
                        df = st.session_state.kw_df
                        try:
                            df_indexed = df.set_index("kwd_no")
                            for kw in unique_kwds:
                                if kw in df_indexed.index:
                                    try:
                                        prev = int(str(df_indexed.at[kw, "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"]).strip() or "0")
                                    except Exception:
                                        prev = 0
                                    df_indexed.at[kw, "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"] = str(prev + 1)
                                    df_indexed.at[kw, "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì"] = now_str
                            st.session_state.kw_df = df_indexed.reset_index()
                        except Exception:
                            for kw in unique_kwds:
                                idx_list = st.session_state.kw_df.index[st.session_state.kw_df["kwd_no"] == kw].tolist()
                                if idx_list:
                                    idx = idx_list[0]
                                    try:
                                        prev = int(str(st.session_state.kw_df.at[idx, "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"]).strip() or "0")
                                    except Exception:
                                        prev = 0
                                    st.session_state.kw_df.at[idx, "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"] = str(prev + 1)
                                    st.session_state.kw_df.at[idx, "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì"] = now_str
                        save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
                        append_hits_log(unique_kwds)

                    out_buf = io.BytesIO()
                    out.to_csv(out_buf, index=False, encoding="utf-8-sig")

                    st.session_state['analysis_df_full'] = out
                    st.session_state['analysis_df_display_unique'] = display_unique
                    st.session_state['analysis_highlight_html'] = highlight_text(text, matches)
                    st.session_state['analysis_count'] = len(display_unique)
                    st.session_state['analysis_text'] = text
                    st.session_state['analysis_csv_bytes'] = out_buf.getvalue()
                    st.session_state['analysis_show'] = True
                else:
                    st.session_state['analysis_show'] = False
                    st.session_state['analysis_df_full'] = None
                    st.session_state['analysis_df_display_unique'] = None
                    st.session_state['analysis_highlight_html'] = ''
                    st.session_state['analysis_count'] = 0
                    st.session_state['analysis_text'] = ''
                    st.session_state['analysis_csv_bytes'] = None
                    st.info("ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    _persist_df_full = st.session_state.get('analysis_df_full')
    _persist_df_unique = st.session_state.get('analysis_df_display_unique')
    _persist_highlight_html = st.session_state.get('analysis_highlight_html', '')
    _persist_count = st.session_state.get('analysis_count')
    _persist_csv = st.session_state.get('analysis_csv_bytes')

    if st.session_state.get('analysis_show') and _persist_df_full is not None:
        from streamlit import column_config as cc
        st.success(f"ì´ {_persist_count}ê±´ì˜ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        show_pos = st.checkbox("ìœ„ì¹˜ ì¸ë±ìŠ¤(start/end) ë³´ê¸°", value=st.session_state.get("cb_pos", False), key="cb_pos")
        base_cols = ["kwd_no", "í‚¤ì›Œë“œëª…", "ìƒí’ˆì¹´í…Œê³ ë¦¬", "ë¦¬ìŠ¤í¬ ë“±ê¸‰", "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€"]
        column_cfg = {"ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€": cc.TextColumn()}

        # DBì˜ 'ëª¨ë“  ì—´'ê³¼ ì¡°ì¸í•˜ì—¬ ì „ì²´ ì •ë³´ í‘œì‹œ
        try:
            merged_full = _persist_df_unique[["kwd_no"]].merge(
                st.session_state.kw_df[get_db_cols()], on="kwd_no", how="left"
            )
            merged_full = sort_for_display(merged_full)
        except Exception:
            merged_full = _persist_df_unique

        if show_pos:
            st.dataframe(_rename_fcol(_persist_df_full[base_cols + ["start", "end"]], width="stretch", column_config=column_cfg))
            st.markdown("**DB ì „ì²´ ì—´ ì •ë³´ (ì¤‘ë³µ ì œê±°)**")
            st.dataframe(_rename_fcol(merged_full), width="stretch")
        else:
            st.dataframe(_rename_fcol(merged_full), width="stretch")

        if _persist_csv is not None:
            st.download_button("ë¶„ì„ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", data=_persist_csv, file_name="analysis_results.csv", mime="text/csv", key="dl_analysis")

        
    st.subheader("í•˜ì´ë¼ì´íŠ¸ ë¯¸ë¦¬ë³´ê¸°")
    # ê¸°ì¡´ í•˜ì´ë¼ì´íŠ¸ HTMLì€ ê·¸ëŒ€ë¡œ í‘œì‹œ
    st.markdown(
        f"<div class='hl-box'>{_persist_highlight_html}</div>",
        unsafe_allow_html=True,
    )

    # ìƒˆ ì°½/ë„¤ë¹„ê²Œì´ì…˜ ì—†ì´ ê°’ë§Œ ì£¼ì…: í•˜ì´ë¼ì´íŠ¸ HTMLì—ì„œ kwd_no ìˆ˜ì§‘ í›„ ë²„íŠ¼ìœ¼ë¡œ ì œê³µ
    def _normalize_kw_label(s: str) -> str:
        try:
            s = str(s)
        except Exception:
            return ""
        # Normalize common invisible spaces (NBSP, zero-width space) to regular space, then strip
        for ch in ("\u00A0", "\u200B"):
            s = s.replace(ch, " ")
        return s.strip()

    _kwd_candidates = []
    try:
        import re as _re_for_kwd
        _raw_kwd_candidates = _re_for_kwd.findall(r"data-kwd='([^']+)'", _persist_highlight_html or "")
        _cleaned_candidates = []
        for _c in _raw_kwd_candidates:
            _n = _normalize_kw_label(_c)
            if _n:
                _cleaned_candidates.append(_n)
        _kwd_candidates = list(dict.fromkeys(_cleaned_candidates))
    except Exception:
        _kwd_candidates = []


    # í•˜ì´ë¼ì´íŠ¸ ë¹ ë¥¸ ì„ íƒ: í† ê¸€ & ì‹œê° í‘œì‹œ(ë²„íŠ¼ ë°°ê²½ìƒ‰ ON/OFF)
    if _kwd_candidates:
        # ğŸ‘‰ í•„í„° ì…ë ¥ì°½ ë‚´ìš©(kwdno_filter_input_tab1)ì„ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ ìƒíƒœ ê³„ì‚°
        _cur_filter = (st.session_state.get("kwdno_filter_input_tab1") or "").strip()
        _raw_selected_list = [p for p in _cur_filter.split(",") if p.strip()]
        selected_list = [_normalize_kw_label(p) for p in _raw_selected_list if _normalize_kw_label(p)]

        with st.expander("í•˜ì´ë¼ì´íŠ¸ ë¹ ë¥¸ ì„ íƒ (í´ë¦­í•˜ë©´ í•„í„° ì…ë ¥ì°½ì— ìë™ ì…ë ¥)", expanded=False):
            st.markdown("<div class='quick-select-block'>", unsafe_allow_html=True)
            # ë²„íŠ¼ ê·¸ë¦¬ë“œ ë°°ì¹˜ (15ì—´ ê³ ì •)
            n = 15
            rows = [_kwd_candidates[i:i+n] for i in range(0, len(_kwd_candidates), n)]
            for row in rows:
                cols = st.columns(n)
                for i, k in enumerate(row):
                    if i >= len(cols):
                        break
                    with cols[i]:
                        # í•„í„° ë¬¸ìì—´ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì„ íƒ(ë¶ˆ ON)
                        is_selected = _normalize_kw_label(k) in selected_list
                        btn_type = "primary" if is_selected else "secondary"
                        if st.button(k, key=f"kwbtn_{k}", type=btn_type):
                            # í˜„ì¬ í•„í„° ê¸°ì¤€ìœ¼ë¡œ ëª©ë¡ ë³µì‚¬
                            parts = selected_list.copy()
                            if is_selected:
                                # ì´ë¯¸ ì„ íƒ â†’ í•„í„°ì—ì„œ ì œê±° (ë¶ˆ OFF)
                                parts = [p for p in parts if p != k]
                            else:
                                # ì„ íƒ ì•ˆ ë¨ â†’ í•„í„°ì— ì¶”ê°€ (ë¶ˆ ON)
                                if k not in parts:
                                    parts.append(k)
                            # í•„í„° ì…ë ¥ì°½ ê°’ ê°±ì‹ 
                            st.session_state["kwdno_filter_input_tab1"] = ", ".join(parts)
                            # ì´í›„ ë£¨í”„ì—ì„œ ë°”ë¡œ ë°˜ì˜ë˜ë„ë¡ ë¡œì»¬ ìƒíƒœë„ ê°±ì‹ 
                            selected_list = parts
            st.markdown("</div>", unsafe_allow_html=True)

            # JS: clicking a highlighted kwd_no writes kwd to query param and reloads SAME window
            st.markdown("""
        <style>
        .sticky-kwd-expander {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            z-index: 999;
            background-color: white;
        }
        </style>
        """, unsafe_allow_html=True)

        st.markdown("""
            <script>
            // í•˜ì´ë¼ì´íŠ¸ í´ë¦­ ì‹œ kwd_noë¥¼ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ ì£¼ì…
            document.addEventListener('click', function(e){
              const el = e.target.closest('.kwdno-link');
              if(!el) return;
              e.preventDefault();
              const kwd = el.getAttribute('data-kwd') || '';
              try {
                const url = new URL(window.location.href);
                url.searchParams.set('kwdno_click', kwd);
                window.location.href = url.toString();
              } catch (err) {
                console.warn('kwdno-click failed', err);
              }
            });

            // "í•˜ì´ë¼ì´íŠ¸ ë¹ ë¥¸ ì„ íƒ" ìµìŠ¤íŒ¬ë”ë¥¼ ìƒë‹¨ì— ê³ ì •
            function markStickyKwdExpander() {
              try {
                const nodes = document.querySelectorAll('[data-testid="stExpander"]');
                nodes.forEach(function(el) {
                  const labelText = (el.innerText || "");
                  if (labelText.indexOf("í•˜ì´ë¼ì´íŠ¸ ë¹ ë¥¸ ì„ íƒ") !== -1) {
                    el.classList.add("sticky-kwd-expander");
                  }
                });
              } catch (err) {
                console.warn('sticky kwd expander failed', err);
              }
            }
            window.addEventListener('load', markStickyKwdExpander);
            setTimeout(markStickyKwdExpander, 1500);
            </script>
            """, unsafe_allow_html=True)


        st.markdown("---")
        st.markdown("#### ğŸ” kwd_no í•„í„°")

        # Read kwd_no injected from highlight click (same-window reload)
        try:
            qp_val = None
            try:
                # Streamlit >= 1.30
                qp = st.query_params
                if isinstance(qp.get("kwdno_click"), list):
                    qp_val = (qp.get("kwdno_click") or [None])[0]
                else:
                    qp_val = qp.get("kwdno_click")
            except Exception:
                # Older Streamlit
                qp = st.experimental_get_query_params()
                qp_val = (qp.get("kwdno_click") or [None])[0]
            if qp_val:
                st.session_state["kwdno_filter_input_tab1"] = str(qp_val)
                # Clear the param to prevent sticky reload
                try:
                    st.query_params.clear()
                except Exception:
                    st.experimental_set_query_params()
        except Exception:
            pass

        def _reset_kwdno_tab1():
            st.session_state["kwdno_filter_input_tab1"] = ""

        kwd_filter_val = st.text_input(
            "kwd_no ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì—¬ëŸ¬ ê°œ ì…ë ¥ ê°€ëŠ¥)",
            placeholder="ì˜ˆ: G060,G105,I200,I201,I202,P024,P025,P027,F031,F131,F180",
            key="kwdno_filter_input_tab1",
        )

        col_f1, col_f2 = st.columns([1, 1])
        with col_f1:
            apply_kwd_filter_tab1 = st.button("í•„í„° ì ìš©", key="btn_apply_kwd_filter_tab1")
        with col_f2:
            reset_kwd_filter_tab1 = st.button("ì´ˆê¸°í™”", key="btn_reset_kwd_filter_tab1", on_click=_reset_kwdno_tab1)

        _base_df = st.session_state.kw_df.copy()
        if apply_kwd_filter_tab1 and st.session_state.kwdno_filter_input_tab1.strip():
            _targets = [x.strip().upper() for x in st.session_state.kwdno_filter_input_tab1.split(",") if x.strip()]
            _view_df = _base_df[_base_df["kwd_no"].astype(str).str.upper().isin(_targets)]
            st.success(f"ì´ {len(_view_df)}ê±´ì´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            _view_df = _base_df

        st.caption(f"í˜„ì¬ í‘œì‹œëœ í•­ëª©: {len(_view_df)} / ì „ì²´: {len(_base_df)}")
        st.dataframe(_rename_fcol(sort_for_display(_view_df[get_db_cols()])), width="stretch")

        buf_bytes = io.BytesIO()
        st.session_state.kw_df[get_db_cols()].to_csv(buf_bytes, index=False, encoding="utf-8-sig")
        st.download_button("í‚¤ì›Œë“œ CSV ë‹¤ìš´ë¡œë“œ", data=buf_bytes.getvalue(), file_name="keywords_current.csv", mime="text/csv", key="dl_kw_current")

with tab2:
    if is_admin():
        st.subheader("í‚¤ì›Œë“œ ê´€ë¦¬ (í¸ì§‘/ì‚­ì œ/í…œí”Œë¦¿/ì—…ë¡œë“œ)")

        st.markdown("### ğŸ“‚ í‚¤ì›Œë“œ ì—…ë¡œë“œ (.csv / .xlsx)")
        col_u1, col_u2 = st.columns([2, 2])
        with col_u1:
            upload_mode = st.radio("ì—…ë¡œë“œ ëª¨ë“œ", ["ê¸°ì¡´ ìœ ì§€ + ìƒˆë¡œ ì¶”ê°€", "ì™„ì „ ë®ì–´ì“°ê¸°"], horizontal=False, key="upload_mode")
        with col_u2:
            uploaded_file = st.file_uploader("í‚¤ì›Œë“œ í…œí”Œë¦¿ ì—…ë¡œë“œ", type=["csv", "xlsx"], key="uploader")

        if uploaded_file is not None:
            try:
                inc_df = None
                name = uploaded_file.name.lower()
                if name.endswith(".csv"):
                    raw = uploaded_file.read()
                    inc_df = read_csv_with_fallback_bytes(raw)
                elif name.endswith(".xlsx"):
                    raw = uploaded_file.read()
                    try:
                        inc_df = pd.read_excel(io.BytesIO(raw), dtype=str).fillna("")
                    except Exception:
                        inc_df = read_xlsx_without_openpyxl(raw, header_row=1)
                else:
                    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” XLSXë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                    inc_df = None

                if inc_df is not None:
                    inc_df = normalize_upload(inc_df)
                    # ê¸°ì¡´ ìœ í‹¸ í•¨ìˆ˜ í´ë°± ì²˜ë¦¬
                    merged = None
                    added_cnt = 0
                    dedup_removed = 0
                    try:
                        inc_df = assign_missing_ids(inc_df)
                        mode = "merge" if upload_mode == "ê¸°ì¡´ ìœ ì§€ + ìƒˆë¡œ ì¶”ê°€" else "overwrite"
                        merged, added_cnt, dedup_removed = merge_or_overwrite(st.session_state.kw_df, inc_df, mode)
                    except NameError:
                        merged = pd.concat([st.session_state.kw_df, inc_df], ignore_index=True)
                    st.session_state.kw_df = merged
                    save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
                    st.success(f"ì—…ë¡œë“œ ì™„ë£Œ â€” ì¶”ê°€ {added_cnt}ê±´, (ë‚´ë¶€ ì¤‘ë³µì œê±° {dedup_removed}ê±´) ì €ì¥ë¨.")
            except Exception as e:
                st.error(f"ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
        st.subheader("í‚¤ì›Œë“œ ì…ë ¥/ê´€ë¦¬")

        c1, c2, c3, c4 = st.columns([2, 2, 2, 1])
        with c1:
            term_in = st.text_input("í‚¤ì›Œë“œëª… *", placeholder="ì˜ˆ: í•„ëŸ¬")
        with c2:
            db_cat_opts = unique_values_from_db("ìƒí’ˆì¹´í…Œê³ ë¦¬")
            cat_choice = st.selectbox("ìƒí’ˆì¹´í…Œê³ ë¦¬", db_cat_opts + ["(ì§ì ‘ ì…ë ¥)"])
        with c3:
            db_risk_opts = unique_values_from_db("ë¦¬ìŠ¤í¬ ë“±ê¸‰")
            risk_choice = st.selectbox("ë¦¬ìŠ¤í¬ ë“±ê¸‰", db_risk_opts)
        with c4:
            add_click = st.button("ì¶”ê°€", type="primary", key="btn_add_row")

        c5, c6, c7 = st.columns([3, 3, 3])
        with c5:
            detail_choice = st.selectbox("ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€", _dropdown_with_input_option(st.session_state.opt_details))
        with c6:
            db_evid_opts = unique_values_from_db("ì¦ë¹™ìë£Œìœ í˜•")
            evidence_choice = st.selectbox("ì¦ë¹™ìë£Œìœ í˜•", _dropdown_with_input_option(db_evid_opts))
        with c7:
            alt_choice = st.selectbox("ëŒ€ì²´í‚¤ì›Œë“œ", _dropdown_with_input_option(st.session_state.opt_alt_terms))

        new_cat = new_detail = new_evid = new_alt = ""
        new_kwd = ""
    
        if cat_choice == "(ì§ì ‘ ì…ë ¥)":
            cols_nc = st.columns([2,1])
            with cols_nc[0]:
                new_cat = st.text_input("ìƒˆ ì¹´í…Œê³ ë¦¬ ì…ë ¥", key="new_cat_input")
            with cols_nc[1]:
                new_kwd = st.text_input("í‚¤ì›Œë“œNO (ì„ íƒ, ì˜ˆ: P001)", key="new_kwd_input")
    
        if detail_choice == "(ì§ì ‘ ì…ë ¥)":
            new_detail = st.text_input("ìƒˆ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€ ì…ë ¥", key="new_detail_input")
        if evidence_choice == "(ì§ì ‘ ì…ë ¥)":
            new_evid = st.text_input("ìƒˆ ì¦ë¹™ìë£Œìœ í˜• ì…ë ¥", key="new_evid_input")
        if alt_choice == "(ì§ì ‘ ì…ë ¥)":
            new_alt = st.text_input("ìƒˆ ëŒ€ì²´í‚¤ì›Œë“œ ì…ë ¥", key="new_alt_input")

        if add_click:
            if not term_in.strip():
                st.warning("í‚¤ì›Œë“œëª…ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
            else:
                category = new_cat.strip() if cat_choice == "(ì§ì ‘ ì…ë ¥)" and new_cat.strip() else cat_choice
                detail = new_detail.strip() if detail_choice == "(ì§ì ‘ ì…ë ¥)" and new_detail.strip() else detail_choice
                evidence = new_evid.strip() if evidence_choice == "(ì§ì ‘ ì…ë ¥)" and new_evid.strip() else evidence_choice
                alt_term = new_alt.strip() if alt_choice == "(ì§ì ‘ ì…ë ¥)" and new_alt.strip() else alt_choice

                if category == "(ì§ì ‘ ì…ë ¥)" or detail == "(ì§ì ‘ ì…ë ¥)" or evidence == "(ì§ì ‘ ì…ë ¥)" or alt_term == "(ì§ì ‘ ì…ë ¥)":
                    st.warning("ìƒˆ í•­ëª©ì„ ì…ë ¥í–ˆìœ¼ë©´ ê°’ì„ ì±„ìš°ê±°ë‚˜ ê¸°ì¡´ ëª©ë¡ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
                else:
                    kwd = None
                    _kw = (st.session_state.get("new_kwd_input", "") or "").strip()
                    if _kw:
                        m = _custom_kwd_pattern.match(_kw)
                        if m:
                            pfx, num = m.group(1).upper(), m.group(2)
                            kwd = f"{pfx}{int(num):0{len(num)}d}"
                    if kwd is None:
                        kwd = next_kwd_no(category)
                    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    _existing_kwds = st.session_state.kw_df['kwd_no'].astype(str).str.upper().tolist()
                    if str(kwd).upper() in _existing_kwds:
                        st.warning(f"{kwd}ì€(ëŠ”) ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë²ˆí˜¸ë¥¼ ì§€ì •í•´ ì£¼ì„¸ìš”.")
                        st.stop()

                    row = {
                        "kwd_no": kwd,
                        "í‚¤ì›Œë“œëª…": term_in.strip(),
                        "ìƒí’ˆì¹´í…Œê³ ë¦¬": category,
                        "ë¦¬ìŠ¤í¬ ë“±ê¸‰": risk_choice,
                        "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€": detail,
                        "ì¦ë¹™ìë£Œìœ í˜•": evidence,
                        "ëŒ€ì²´í‚¤ì›Œë“œ": alt_term,
                        "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)": "0",
                        "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì": "",

                        "í‚¤ì›Œë“œ ë“±ë¡ì¼ì": now_str
                    }
                    st.session_state.kw_df = pd.concat([st.session_state.kw_df, pd.DataFrame([row])], ignore_index=True)
                    save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
                    st.success(f"[{kwd}] '{term_in}' ì¶”ê°€ ë° ì €ì¥ë¨")

                    # ì €ì¥ ì„±ê³µ í›„ì—ë§Œ ë“œë¡­ë‹¤ìš´ ì˜µì…˜ì— ì‹ ê·œ ê°’ ì¶”ê°€ ë° JSON ì €ì¥
                    if cat_choice == "(ì§ì ‘ ì…ë ¥)" and new_cat.strip() and new_cat not in st.session_state.opt_categories:
                        st.session_state.opt_categories.append(new_cat.strip())
                        save_dropdown_config()
                    if detail_choice == "(ì§ì ‘ ì…ë ¥)" and new_detail.strip() and new_detail not in st.session_state.opt_details:
                        st.session_state.opt_details.append(new_detail.strip())
                        save_dropdown_config()
                    if evidence_choice == "(ì§ì ‘ ì…ë ¥)" and new_evid.strip() and new_evid not in st.session_state.opt_evidences:
                        st.session_state.opt_evidences.append(new_evid.strip())
                        save_dropdown_config()
                    if alt_choice == "(ì§ì ‘ ì…ë ¥)" and new_alt.strip() and new_alt not in st.session_state.opt_alt_terms:
                        st.session_state.opt_alt_terms.append(new_alt.strip())
                        save_dropdown_config()

        with st.expander("ë“œë¡­ë‹¤ìš´ ê°’ ì‚­ì œ (ê´€ë¦¬ì)"):
            colm1, colm2 = st.columns([2,3])

            with colm1:
                target_list = st.selectbox("ëŒ€ìƒ ëª©ë¡", [
                    "ìƒí’ˆì¹´í…Œê³ ë¦¬", "ë¦¬ìŠ¤í¬ ë“±ê¸‰", "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€", "ì¦ë¹™ìë£Œìœ í˜•", "ëŒ€ì²´í‚¤ì›Œë“œ"
                ], key="del_list")

            with colm2:
                options_map = {
                    "ìƒí’ˆì¹´í…Œê³ ë¦¬": st.session_state.opt_categories,
                    "ë¦¬ìŠ¤í¬ ë“±ê¸‰": st.session_state.opt_risks,
                    "ë¦¬ìŠ¤í¬ ë“±ê¸‰ë³„ ì„¸ë¶€ ì‹¬ì˜ê¸°ì¤€": st.session_state.opt_details,
                    "ì¦ë¹™ìë£Œìœ í˜•": st.session_state.opt_evidences,
                    "ëŒ€ì²´í‚¤ì›Œë“œ": st.session_state.opt_alt_terms,
                }

                current = options_map.get(target_list, [])
                to_del = st.selectbox("ì‚­ì œí•  ê°’ ì„ íƒ", current, key="del_value") if current else None

            if to_del is not None and st.button("ì‚­ì œ", key="btn_del_value"):
                st.session_state['__del_request__'] = (target_list, to_del)

            if st.session_state.get('__del_request__'):
                tgt, val = st.session_state['__del_request__']
                st.warning(f"{tgt}ì—ì„œ '{val}' ê°’ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                cols_confirm = st.columns([1,1,6])

                if cols_confirm[0].button("ì˜ˆ, ì‚­ì œ", key="btn_del_confirm"):
                    lst = options_map[tgt]
                    try:
                        lst.remove(val)
                        save_dropdown_config()  # JSON íŒŒì¼ì— ì €ì¥
                        st.success(f"ì‚­ì œ ì™„ë£Œ: {tgt} â†’ {val}")
                    except ValueError:
                        st.info("ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    st.session_state['__del_request__'] = None

                if cols_confirm[1].button("ì•„ë‹ˆì˜¤", key="btn_del_cancel"):
                    st.session_state['__del_request__'] = None

        st.markdown("---")

        mode = st.radio("í¸ì§‘ ëª¨ë“œ ì„ íƒ", ["ë“œë¡­ë‹¤ìš´(ì œí•œ ì…ë ¥)", "ììœ  ì…ë ¥"], horizontal=True, key="edit_mode")
        base_df = sort_for_display(st.session_state.kw_df[get_db_cols()].copy())

        if mode == "ë“œë¡­ë‹¤ìš´(ì œí•œ ì…ë ¥)":
            from streamlit import column_config as cc
            # Build union option lists including existing DB values
            db_cat_opts = unique_values_from_db("ìƒí’ˆì¹´í…Œê³ ë¦¬")
            db_risk_opts = unique_values_from_db("ë¦¬ìŠ¤í¬ ë“±ê¸‰")
            db_detail_opts = unique_values_from_db(RAW_F_COL)
            db_alt_opts = unique_values_from_db("ëŒ€ì²´í‚¤ì›Œë“œ")
            detail_opts = _dropdown_with_input_option(list(set((st.session_state.opt_details or []) + (db_detail_opts or []))))
            alt_opts = _dropdown_with_input_option(list(set((st.session_state.opt_alt_terms or []) + (db_alt_opts or []))))
            risk_opts = list(set((st.session_state.opt_risks or []) + (db_risk_opts or []))) or st.session_state.opt_risks
            cat_opts = list(set((st.session_state.opt_categories or []) + (db_cat_opts or []))) or st.session_state.opt_categories
            edited_df = st.data_editor(_rename_fcol(
                base_df),
                column_config={
                    "ìƒí’ˆì¹´í…Œê³ ë¦¬": cc.SelectboxColumn(options=cat_opts, required=False),
                    "ë¦¬ìŠ¤í¬ ë“±ê¸‰": cc.SelectboxColumn(options=risk_opts, required=False),
                    DISPLAY_F_COL: cc.SelectboxColumn(options=detail_opts, required=False, label=DISPLAY_F_COL),
                    "ì¦ë¹™ìë£Œìœ í˜•": cc.SelectboxColumn(options=st.session_state.opt_evidences, required=False),
                    "ëŒ€ì²´í‚¤ì›Œë“œ": cc.SelectboxColumn(options=alt_opts, required=False),
                },
                width="stretch",
                num_rows="dynamic",
                key="editor_dropdown",
            )
        else:
            edited_df = st.data_editor(_rename_fcol(base_df), width="stretch", num_rows="dynamic", key="editor_free")

        csave, cdel, ctmpl = st.columns([1, 1, 2])
        with csave:
            if st.button("ë³€ê²½ì‚¬í•­ ì €ì¥", type="primary", key="btn_save_edits"):
                edited_df = edited_df.fillna("")
                # --- Map UI display column back to raw DB column before slicing ---
                try:
                    if DISPLAY_F_COL in edited_df.columns and RAW_F_COL not in edited_df.columns:
                        edited_df[RAW_F_COL] = edited_df[DISPLAY_F_COL]
                        try:
                            edited_df = edited_df.drop(columns=[DISPLAY_F_COL])
                        except Exception:
                            pass
                except Exception:
                    pass
                try:
                    st.session_state.kw_df = edited_df[get_db_cols()].copy()
                except KeyError:
                    missing = [c for c in get_db_cols() if c not in edited_df.columns]
                    for c in missing:
                        edited_df[c] = ""
                    st.session_state.kw_df = edited_df[get_db_cols()].copy()

                save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
                st.success("ìˆ˜ì • ë‚´ìš© ì €ì¥ ì™„ë£Œ")

        with cdel:
            del_targets = st.multiselect(
                "ì‚­ì œí•  í•­ëª© ì„ íƒ (kwd_no ê¸°ì¤€)",
                options=st.session_state.kw_df["kwd_no"].tolist(),
                key="del_targets"
            )
            if st.button("ì„ íƒ ì‚­ì œ", type="secondary", key="btn_delete_rows"):
                if not del_targets:
                    st.warning("ì‚­ì œí•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”.")
                else:
                    st.session_state.kw_df = st.session_state.kw_df[~st.session_state.kw_df["kwd_no"].isin(del_targets)].reset_index(drop=True)
                    save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
                    st.success(f"{len(del_targets)}ê±´ ì‚­ì œ ì™„ë£Œ")

        with ctmpl:
            st.markdown("**ì—…ë¡œë“œ í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ**")
            tmpl_csv_buf = io.BytesIO()
            pd.DataFrame(columns=DB_COLS).to_csv(tmpl_csv_buf, index=False, encoding="utf-8-sig")
            st.download_button("CSV í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ", data=tmpl_csv_buf.getvalue(), file_name="keyword_template.csv", mime="text/csv", key="dl_tmpl_csv")
            try:
                tmpl_xlsx_buf = io.BytesIO()
                with pd.ExcelWriter(tmpl_xlsx_buf) as writer:
                    pd.DataFrame(columns=DB_COLS).to_excel(writer, sheet_name="keywords_template", index=False)
                st.download_button("ì—‘ì…€ í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ", data=tmpl_xlsx_buf.getvalue(), file_name="keyword_template.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key="dl_tmpl_xlsx")
            except Exception as e:
                st.info(f"ì—‘ì…€ í…œí”Œë¦¿ì€ í˜„ì¬ í™˜ê²½ì— openpyxl/xlsxwriterê°€ ì—†ì–´ CSV í…œí”Œë¦¿ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. (ì„¸ë¶€: {e})")

        st.caption(f"í˜„ì¬ ì €ì¥ëœ í•­ëª©: {len(st.session_state.kw_df)}")
        st.dataframe(_rename_fcol(sort_for_display(st.session_state.kw_df[get_db_cols()])), width="stretch")


    else:
        st.subheader("í‚¤ì›Œë“œ ê´€ë¦¬ (ê´€ë¦¬ì ì „ìš©)")
        st.info("ì´ íƒ­ì€ ê´€ë¦¬ì ì „ìš©ì…ë‹ˆë‹¤. ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì˜ 'ê´€ë¦¬ì ë¡œê·¸ì¸' ì˜ì—­ì—ì„œ ì¸ì¦ í›„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

with tab3:
    st.subheader("ğŸ“ˆ ë¦¬í¬íŠ¸ â€” ìµœê·¼ 7ì¼ Top N")

    # ë¦¬í¬íŠ¸ ë³¸ë¬¸
    log_df = load_hits_log()
    if log_df.empty:
        st.info("ì¡°íšŒ ë¡œê·¸ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ëª‡ ë²ˆ ì‹¤í–‰í•˜ë©´ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë©ë‹ˆë‹¤.")
    else:
        col_r1, col_r2 = st.columns([1,1])
        with col_r1:
            top_n = st.number_input("Top N", min_value=1, max_value=100, value=10, step=1, key="report_topn")
        with col_r2:
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=7)
            st.write(f"ê¸°ê°„: {start_dt.strftime('%Y-%m-%d %H:%M:%S')} ~ {end_dt.strftime('%Y-%m-%d %H:%M:%S')}")

        try:
            _tmp = log_df.copy()
            _tmp["ts_dt"] = pd.to_datetime(_tmp["ts"], errors="coerce")
            mask = (_tmp["ts_dt"] >= start_dt) & (_tmp["ts_dt"] <= end_dt)
            week_df = _tmp.loc[mask]
            agg = week_df.groupby("kwd_no").size().reset_index(name="ìµœê·¼7ì¼_ì¡°íšŒìˆ˜")
            agg = agg.sort_values("ìµœê·¼7ì¼_ì¡°íšŒìˆ˜", ascending=False).head(top_n)

            report = agg.merge(st.session_state.kw_df[get_db_cols()], on="kwd_no", how="left")
            report = report.sort_values(["ìµœê·¼7ì¼_ì¡°íšŒìˆ˜","kwd_no"], ascending=[False, True], kind="mergesort")

            # ---- Column order tweak: place 'ìµœê·¼7ì¼_ì¡°íšŒìˆ˜' right after 'ëŒ€ì²´í‚¤ì›Œë“œ' ----
            try:
                cols = list(report.columns)
                if "ìµœê·¼7ì¼_ì¡°íšŒìˆ˜" in cols:
                    cols.remove("ìµœê·¼7ì¼_ì¡°íšŒìˆ˜")
                    if "ëŒ€ì²´í‚¤ì›Œë“œ" in cols:
                        idx = cols.index("ëŒ€ì²´í‚¤ì›Œë“œ") + 1
                    elif "ì¦ë¹™ìë£Œìœ í˜•" in cols:
                        idx = cols.index("ì¦ë¹™ìë£Œìœ í˜•") + 1
                    else:
                        idx = len(cols)
                    cols.insert(idx, "ìµœê·¼7ì¼_ì¡°íšŒìˆ˜")
                    report = report[cols]
            except Exception:
                pass
            # ----------------------------------------------------------------------

            # ---- Right align numeric columns ----
            align_cols = [c for c in ["ìµœê·¼7ì¼_ì¡°íšŒìˆ˜", "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"] if c in report.columns]
            styled = report.style.set_properties(**{"text-align": "right"}, subset=align_cols)
            
            st.dataframe(_rename_fcol(styled), width="stretch")
            rep_buf = io.BytesIO()
            report.to_csv(rep_buf, index=False, encoding="utf-8-sig")
            st.download_button("ë¦¬í¬íŠ¸ CSV ë‹¤ìš´ë¡œë“œ (ìµœê·¼ 7ì¼ Top N)", data=rep_buf.getvalue(), file_name="weekly_topN_report.csv", mime="text/csv", key="dl_week_report")
        except Exception as e:
            st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    # ------------------------------
    # ğŸ” ì¡°íšŒ ìˆ˜ ì´ˆê¸°í™” (CSV ë‹¤ìš´ë¡œë“œ ì˜ì—­ 'ì•„ë˜'ë¡œ ì´ë™)
    # ------------------------------
    st.markdown("---")
    st.markdown("### ğŸ” ì¡°íšŒ ìˆ˜ ì´ˆê¸°í™”")

    # ì „ì²´ ì´ˆê¸°í™”: DBì˜ ëˆ„ì ì¹´ìš´íŠ¸=0, ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì ê³µë°± (ë¡œê·¸ëŠ” ìœ ì§€ â€” ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    col_z1, col_z2 = st.columns([1,1])
    with col_z1:
        confirm_reset_all = st.checkbox("ì „ì²´ ì¡°íšŒ ìˆ˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³  'ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì'ë¥¼ ë¹„ìš°ê¸°", value=False, key="report_reset_ck")
        if st.button("ì „ì²´ ì´ˆê¸°í™” ì‹¤í–‰", disabled=not confirm_reset_all, key="report_reset_btn"):
            try:
                if "í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)" in st.session_state.kw_df.columns:
                    st.session_state.kw_df["í‚¤ì›Œë“œ ì¡°íšŒ ìˆ˜(ëˆ„ì ì¹´ìš´íŠ¸)"] = "0"
                if "ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì" in st.session_state.kw_df.columns:
                    st.session_state.kw_df["ë§ˆì§€ë§‰ ì¶œë ¥ì¼ì"] = ""
                save_db(st.session_state.kw_df, Path(st.session_state.storage_path))
                st.success("ì „ì²´ ì´ˆê¸°í™” ì™„ë£Œ (ë¡œê·¸ íŒŒì¼ì€ ìœ ì§€ë©ë‹ˆë‹¤)")
            except Exception as e:
                st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ìµœê·¼ 7ì¼ ì´ˆê¸°í™”: ë¡œê·¸ì—ì„œ ìµœê·¼ 7ì¼ ê¸°ë¡ë§Œ ì‚­ì œ (DB ëˆ„ì ì¹´ìš´íŠ¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    with col_z2:
        confirm_reset_week = st.checkbox("ìµœê·¼ 7ì¼ ì¡°íšŒìˆ˜ ì´ˆê¸°í™” (ë¡œê·¸ì—ì„œ ìµœê·¼ 7ì¼ ê¸°ë¡ ì‚­ì œ)", value=False, key="report_reset7_ck")
        if st.button("ìµœê·¼ 7ì¼ ì´ˆê¸°í™” ì‹¤í–‰", disabled=not confirm_reset_week, key="report_reset7_btn"):
            try:
                log_df2 = load_hits_log()
                if log_df2.empty:
                    st.info("ì‚­ì œí•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    end_dt2 = datetime.now()
                    start_dt2 = end_dt2 - timedelta(days=7)
                    log_df2["ts_dt"] = pd.to_datetime(log_df2["ts"], errors="coerce")
                    before_cnt = len(log_df2)
                    # keep records outside the last 7 days or with NaT (safety)
                    keep_mask = (log_df2["ts_dt"].isna()) | (log_df2["ts_dt"] < start_dt2) | (log_df2["ts_dt"] > end_dt2)
                    new_log = log_df2.loc[keep_mask, ["ts","kwd_no"]].reset_index(drop=True)
                    overwrite_hits_log(new_log)
                    removed = before_cnt - len(new_log)
                    st.success(f"ìµœê·¼ 7ì¼ ë¡œê·¸ {removed}ê±´ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                st.error(f"ìµœê·¼ 7ì¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")